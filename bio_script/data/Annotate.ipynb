{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before InitGoogleLogging() is written to STDERR\n",
      "I0801 08:07:49.115695 95074 95074 global_state_accessor.cc:25] Redis server address = 172.31.42.10:6379, is test flag = 0\n",
      "I0801 08:07:49.138334 95074 95074 redis_client.cc:146] RedisClient connected.\n",
      "I0801 08:07:49.147186 95074 95074 redis_gcs_client.cc:89] RedisGcsClient Connected.\n",
      "I0801 08:07:49.148728 95074 95074 service_based_gcs_client.cc:193] Reconnected to GCS server: 172.31.42.10:43195\n",
      "I0801 08:07:49.148934 95074 95074 service_based_accessor.cc:92] Reestablishing subscription for job info.\n",
      "I0801 08:07:49.148941 95074 95074 service_based_accessor.cc:422] Reestablishing subscription for actor info.\n",
      "I0801 08:07:49.148947 95074 95074 service_based_accessor.cc:797] Reestablishing subscription for node info.\n",
      "I0801 08:07:49.148950 95074 95074 service_based_accessor.cc:1073] Reestablishing subscription for task info.\n",
      "I0801 08:07:49.148955 95074 95074 service_based_accessor.cc:1248] Reestablishing subscription for object locations.\n",
      "I0801 08:07:49.148959 95074 95074 service_based_accessor.cc:1368] Reestablishing subscription for worker failures.\n",
      "I0801 08:07:49.148967 95074 95074 service_based_gcs_client.cc:86] ServiceBasedGcsClient Connected.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from itertools import chain\n",
    "from tqdm import tqdm\n",
    "import ray\n",
    "ray.init(ignore_reinit_error=True, address=\"auto\")\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(seed=0)\n",
    "\n",
    "sampling_rate_no_weak_labels = 0.1\n",
    "\n",
    "num = 0\n",
    "all_text = []\n",
    "with open('all_text.txt', 'r') as f:\n",
    "    for l in f:\n",
    "        l = l.strip()\n",
    "        if l == \"\":\n",
    "            continue\n",
    "        all_text.append(l)\n",
    "        num += 1\n",
    "\n",
    "# Change the TGT_ENTITY_TYPE for generating weakly supervised data for different entity\n",
    "TGT_ENTITY_TYPE = 'Disease'\n",
    "TGT_ENTITY_TYPE = 'Chemical'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Formate assay in body fluids: application in methanol poisoning.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3016\n",
      "5827\n"
     ]
    }
   ],
   "source": [
    "with open('disease_dict.txt', 'r') as f:\n",
    "    dict_disease = [x.strip() for x in f if x.strip() != \"\"]\n",
    "with open('chem_dict.txt', 'r') as f:\n",
    "    dict_chem = [x.strip() for x in f if x.strip() != \"\"]\n",
    "print(len(dict_chem))\n",
    "print(len(dict_disease))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def f(text_lines):\n",
    "    labeled_lines = []\n",
    "    unlabeled_lines = []\n",
    "\n",
    "    for line in text_lines:\n",
    "        labels = [\"O\"]*len(line)\n",
    "        if TGT_ENTITY_TYPE == \"Chemical\":\n",
    "            entities = dict_chem\n",
    "        if TGT_ENTITY_TYPE == \"Disease\":\n",
    "            entities = dict_disease\n",
    "        entity_type = TGT_ENTITY_TYPE\n",
    "\n",
    "        for entity in entities:\n",
    "            en_len = len(entity)\n",
    "            p = line.find(entity)\n",
    "            while p != -1:\n",
    "                if (p>0 and line[p-1].isalnum()) or (p+en_len<len(line) and line[p+en_len].isalnum()):\n",
    "                    # only part of string skip\n",
    "                    pass\n",
    "                elif all([l=='O' for l in labels[p:p+en_len]]):\n",
    "                    for i in range(1, en_len):\n",
    "                        labels[p+i] = 'I-'+entity_type\n",
    "                    labels[p] = 'B-'+entity_type\n",
    "                    if p>0 and line[p-1] == \"-\":\n",
    "                        pp = p-1\n",
    "                        while pp >= 0 and (line[pp].isalnum() or line[pp] == '-'):\n",
    "                            assert labels[pp]=='O', f\"AS1\\n{line[p:p+en_len]}\\n{labels[p:p+en_len]}\\n{line[pp:p+en_len]}\\n{labels[pp:p+en_len]}\\n{line[:p+en_len]}\\n{labels[:p+en_len]}\"\n",
    "                            labels[pp+1] = 'I-'+entity_type\n",
    "                            labels[pp] = 'B-'+entity_type\n",
    "                            pp -= 1\n",
    "                    if p+en_len<len(line) and line[p+en_len] == \"-\":\n",
    "                        pp = p+en_len\n",
    "                        while pp<len(line) and (line[pp].isalnum() or line[pp] == '-'):\n",
    "#                             assert labels[pp]=='O', f\"AS2\\n{line[p:p+en_len]}\\n{labels[p:p+en_len]}\\n{line[p:pp]}\\n{labels[p:pp]}\\n{line[:pp]}\\n{labels[:pp]}\"\n",
    "                            labels[pp] = 'I-'+entity_type\n",
    "                            pp += 1\n",
    "                            \n",
    "                elif all([l[0]=='I' for l in labels[p:p+en_len]]):\n",
    "                    # contained\n",
    "                    pass\n",
    "                elif labels[p][0] == 'B' and all([l[0]=='I' for l in labels[p+1:p+en_len]]):\n",
    "                    # contained\n",
    "                    pass\n",
    "                elif all([l in ['O', 'I-'+entity_type] for l in labels[p:p+en_len]]):\n",
    "                    # partially contained, extend current entity\n",
    "                    for i in range(en_len):\n",
    "                        labels[p+i] = 'I-'+entity_type\n",
    "                    if p+en_len<len(line) and line[p+en_len] == \"-\":\n",
    "                        pp = p+en_len\n",
    "                        while pp<len(line) and (line[pp].isalnum() or line[pp] == '-'):\n",
    "#                             assert labels[pp]=='O', f\"AS3\\n{line[p:p+en_len]}\\n{labels[p:p+en_len]}\\n{line[p:pp]}\\n{labels[p:pp]}\\n{line[:pp]}\\n{labels[:pp]}\"\n",
    "                            labels[pp] = 'I-'+entity_type\n",
    "                            pp += 1\n",
    "                elif all([l in ['O', 'I-'+entity_type, 'B-'+entity_type] for l in labels[p:p+en_len]]):\n",
    "                    # partially contained, extend current entity\n",
    "                    for i in range(1,en_len):\n",
    "                        labels[p+i] = 'I-'+entity_type\n",
    "                        \n",
    "                    if p+en_len<len(line) and line[p+en_len] == \"-\" and labels[p+en_len]=='O':\n",
    "                        pp = p+en_len\n",
    "                        while pp<len(line) and (line[pp].isalnum() or line[pp] == '-'):\n",
    "#                             assert labels[pp]=='O', f\"AS4\\n{line[p:p+en_len]}\\n{labels[p:p+en_len]}\\n{line[p:pp]}\\n{labels[p:pp]}\\n{line[:pp]}\\n{labels[:pp]}\"\n",
    "                            labels[pp] = 'I-'+entity_type\n",
    "                            pp += 1\n",
    "                            \n",
    "                    if labels[p] != 'I-'+entity_type:\n",
    "                        labels[p] = 'B-'+entity_type\n",
    "                        if p>0 and line[p-1] == \"-\":\n",
    "                            pp = p-1\n",
    "                            while pp >= 0 and (line[pp].isalnum() or line[pp] == '-'):\n",
    "                                assert labels[pp]=='O', f\"AS5\\n{line[p:p+en_len]}\\n{labels[p:p+en_len]}\\n{line[pp:p+en_len]}\\n{labels[pp:p+en_len]}\\n{line[:p+en_len]}\\n{labels[:p+en_len]}\"\n",
    "                                labels[pp+1] = 'I-'+entity_type\n",
    "                                labels[pp] = 'B-'+entity_type\n",
    "                                pp -= 1\n",
    "                else:\n",
    "                    assert False, f\"AS6\\nsomething wrong\\n{labels[p:p+en_len]} \\n{line[p:p+en_len]}\\t{entity_type}\\n{line}\\n{labels}\"\n",
    "                p = line.find(entity, p+1)\n",
    "        if all([l=='O' for l in labels]):\n",
    "            unlabeled_lines.append([line, labels])\n",
    "        else:\n",
    "            labeled_lines.append([line, labels])\n",
    "    \n",
    "    return labeled_lines, unlabeled_lines\n",
    "\n",
    "# labeled_lines,unlabeled_lines = f(all_text)\n",
    "    \n",
    "num_chunks = 500\n",
    "chunk_size = len(all_text)//num_chunks + 1\n",
    "all_processed_data = ray.get([f.remote(all_text[chunk_size*i: min(len(all_text), chunk_size*(i+1))]) for i in range(num_chunks)])\n",
    "labeled_lines = list(chain.from_iterable([x[0] for x in all_processed_data]))\n",
    "unlabeled_lines = list(chain.from_iterable([x[1] for x in all_processed_data]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225194\n",
      "637757\n"
     ]
    }
   ],
   "source": [
    "print(len(labeled_lines))\n",
    "print(len(unlabeled_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('labeled_lines.pickle', 'wb') as handle:\n",
    "    pickle.dump(labeled_lines, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('unlabeled_lines.pickle', 'wb') as handle:\n",
    "    pickle.dump(unlabeled_lines, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 225194/225194 [00:59<00:00, 3770.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of sentences:  435715\n",
      "Example:  [('Delineation', 'O'), ('of', 'O'), ('the', 'O'), ('intimate', 'O'), ('details', 'O'), ('of', 'O'), ('the', 'O'), ('backbone', 'O'), ('conformation', 'O'), ('of', 'O'), ('pyridine', 'O'), ('nucleotide', 'B-Chemical'), ('coenzymes', 'O'), ('in', 'O'), ('aqueous', 'O'), ('solution', 'O'), ('.', 'O')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## Tokenize\n",
    "\n",
    "import nltk.data\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "from nltk.corpus import wordnet\n",
    "from collections import defaultdict\n",
    "from collections import OrderedDict\n",
    "from tqdm import tqdm\n",
    "\n",
    "import re\n",
    "TOKENIZATION_REGEXS = OrderedDict([\n",
    "    # NERsuite-like tokenization: alnum sequences preserved as single\n",
    "    # tokens, rest are single-character tokens.\n",
    "    ('default', re.compile(r'([^\\W_]+|.)')),\n",
    "    # Finer-grained tokenization: also split alphabetical from numeric.\n",
    "    ('fine', re.compile(r'([0-9]+|[^\\W0-9_]+|.)')),\n",
    "    # Whitespace tokenization\n",
    "    ('space', re.compile(r'(\\S+)')),\n",
    "])\n",
    "\n",
    "\n",
    "def sentence_to_tokens(text, tokenization_re=None):\n",
    "    \"\"\"Return list of tokens in given sentence using NERsuite tokenization.\"\"\"\n",
    "\n",
    "    if tokenization_re is None:\n",
    "        tokenization_re = TOKENIZATION_REGEXS.get('default')\n",
    "    tok = [t for t in tokenization_re.split(text) if t]\n",
    "    assert ''.join(tok) == text\n",
    "    return tok\n",
    "\n",
    "def span_anno2single_anno(token, anno):\n",
    "    is_O = all([a=='O' for a in anno])\n",
    "    if is_O:\n",
    "        return 'O'\n",
    "    is_B = all([a[0]=='I' for a in anno[1:]]) and anno[0][0] == 'B'\n",
    "    if is_B:\n",
    "        return anno[0]\n",
    "    is_I = all([a[0]=='I' for a in anno])\n",
    "    if is_I:\n",
    "        return anno[0]\n",
    "    print(f\"{anno}\\n{token}\")\n",
    "    \n",
    "    pos_l = set()\n",
    "    for tmp in wordnet.synsets(token):\n",
    "        if tmp.name().split('.')[0] == token:\n",
    "            pos_l.add(tmp.pos())\n",
    "    pos_l = list(pos_l)\n",
    "#     print(pos_l[0])\n",
    "    if len(pos_l) == 1 and pos_l[0] in ['a', 's']:\n",
    "        print(f\"===> O (pos: {pos_l[0]})\")\n",
    "        return \"O\"\n",
    "    if len(pos_l) != 0:\n",
    "        print(f\"  (pos: {pos_l})\")\n",
    "        \n",
    "    if any([a[0]=='B' for a in anno]):\n",
    "        for a in anno:\n",
    "            if a[0] == 'B':\n",
    "                print(\"===> \", a)\n",
    "                return a\n",
    "    elif anno[0][0] == 'I':\n",
    "        print(\"===> \", anno[0])\n",
    "        return anno[0]\n",
    "    else:\n",
    "        assert False\n",
    "        \n",
    "\n",
    "# @ray.remote\n",
    "def f(text_lines):\n",
    "    all_samples = []\n",
    "    for text,anno in tqdm(text_lines):\n",
    "        # doc2sentence\n",
    "        sentence_spans = [list(s) for s in sent_detector.span_tokenize(text)]\n",
    "        new_sentence_spans = []\n",
    "        for span_id, span in enumerate(sentence_spans):\n",
    "            if span[1] < len(text)-1:\n",
    "                if anno[span[1]] != \"O\":\n",
    "                    print(f\"\\'{text[span[1]+1]}\\', anno: {anno[span[1]+1]} \\n {text[span[0]:span[1]]} \\n {span} \\n {sentence_spans}\")\n",
    "                    span[1] = sentence_spans[span_id+1][1]\n",
    "                    sentence_spans[span_id+1][0] = span[0]\n",
    "                    continue\n",
    "            new_sentence_spans.append(span)\n",
    "\n",
    "        sentence_spans = new_sentence_spans\n",
    "        text_anno_pairs = [(text[span[0]:span[1]], anno[span[0]:span[1]]) for span in sentence_spans ]\n",
    "        for sent, s_anno in text_anno_pairs:\n",
    "            tokens = sentence_to_tokens(sent)\n",
    "            offset = 0\n",
    "            token_anno_pairs = []\n",
    "            prev_t_anno = 'O'\n",
    "\n",
    "            if all([a==\"O\" for a in s_anno]):\n",
    "                continue\n",
    "\n",
    "            for t in tokens:\n",
    "                if not t.isspace():\n",
    "                    t_anno = s_anno[offset: offset+len(t)]\n",
    "                    t_anno = span_anno2single_anno(t,t_anno)\n",
    "                    if t_anno[0] == 'I':\n",
    "                        assert prev_t_anno == t_anno or prev_t_anno == t_anno.replace(\"I-\",\"B-\"), f\"{token_anno_pairs[-1]}, {(t,t_anno)}\"\n",
    "                    prev_t_anno = t_anno\n",
    "                    token_anno_pairs.append((t,t_anno))\n",
    "                offset += len(t)\n",
    "            all_samples.append(token_anno_pairs)\n",
    "#             if len(all_samples) % 10000==0:\n",
    "#                 print(len(all_samples))\n",
    "    return all_samples\n",
    "\n",
    "# num_chunks = 1000\n",
    "# chunk_size = len(labeled_lines)//num_chunks + 1\n",
    "# all_samples = ray.get([f.remote(labeled_lines[chunk_size*i: min(len(labeled_lines), chunk_size*(i+1))]) for i in range(num_chunks)])\n",
    "# all_samples = list(chain.from_iterable([x[0] for x in all_processed_data]))\n",
    "all_samples = f(labeled_lines)\n",
    "\n",
    "print(\"Num of sentences: \", len(all_samples))\n",
    "print(\"Example: \", all_samples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Samples:  435715\n",
      "Example: \n",
      " [('Effects', 'O'), ('of', 'O'), ('5', 'O'), (',', 'O'), ('6', 'O'), ('-', 'O'), ('dihydroxytryptamine', 'O'), ('on', 'O'), ('tyrosine', 'B-Chemical'), ('-', 'I-Chemical'), ('hydroxylase', 'I-Chemical'), ('activity', 'O'), ('in', 'O'), ('central', 'O'), ('catecholaminergic', 'O'), ('neurons', 'O'), ('of', 'O'), ('the', 'O'), ('rat', 'O'), ('.', 'O')]\n",
      "Example: \n",
      " [('The', 'O'), ('spontaneous', 'O'), ('inactivation', 'O'), ('of', 'O'), ('yeast', 'O'), ('glyceraldehyde', 'B-Chemical'), ('-', 'I-Chemical'), ('3', 'I-Chemical'), ('-', 'I-Chemical'), ('phosphate', 'I-Chemical'), ('dehydrogenase', 'O'), ('was', 'O'), ('found', 'O'), ('to', 'O'), ('fit', 'O'), ('a', 'O'), ('simple', 'O'), ('two', 'O'), ('-', 'O'), ('state', 'O'), ('model', 'O'), ('at', 'O'), ('pH', 'O'), ('8', 'O'), ('.', 'O'), ('5', 'O'), ('and', 'O'), ('25', 'O'), ('degrees', 'O'), ('.', 'O')]\n",
      "Example: \n",
      " [('It', 'O'), ('is', 'O'), ('suggested', 'O'), ('that', 'O'), ('calcium', 'B-Chemical'), ('-', 'I-Chemical'), ('stone', 'I-Chemical'), ('formation', 'O'), ('in', 'O'), ('humans', 'O'), ('is', 'O'), ('represented', 'O'), ('by', 'O'), ('two', 'O'), ('different', 'O'), ('populations', 'O'), ('.', 'O')]\n",
      "Example: \n",
      " [('In', 'O'), ('contrast', 'B-Chemical'), ('to', 'O'), ('the', 'O'), ('yeast', 'O'), ('enzyme', 'O'), ('the', 'O'), ('Arrhenius', 'O'), ('plot', 'O'), ('is', 'O'), ('linear', 'O'), ('and', 'O'), (',', 'O'), ('therefore', 'O'), (',', 'O'), ('the', 'O'), ('beef', 'O'), ('liver', 'O'), ('enzyme', 'O'), ('is', 'O'), ('not', 'O'), ('transformed', 'O'), ('into', 'O'), ('an', 'O'), ('inactive', 'O'), ('conformation', 'O'), ('at', 'O'), ('low', 'O'), ('temperatures', 'O'), ('.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "print(\"# of Samples: \", len(all_samples))\n",
    "print(\"Example: \\n\", all_samples[10])\n",
    "print(\"Example: \\n\", all_samples[100])\n",
    "print(\"Example: \\n\", all_samples[1000])\n",
    "print(\"Example: \\n\", all_samples[10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = f'weak.txt'\n",
    "# with open(save_path, 'w') as f:\n",
    "#     for s in all_samples:\n",
    "#         f.write(f\"aps\\tB-category\\n\")\n",
    "#         for t,t_anno in s:\n",
    "#             f.write(f\"{t}\\t{t_anno}\\n\")\n",
    "#         f.write(\"\\n\")\n",
    "if TGT_ENTITY_TYPE == \"Chemical\":\n",
    "    with open(\"chem_\"+save_path, 'w') as f:\n",
    "        for s in all_samples:\n",
    "            if not any([e[1]==\"B-Chemical\" for e in s]):\n",
    "                continue\n",
    "            for t,t_anno in s:\n",
    "                f.write(f'{t}\\t{t_anno.replace(\"B-Disease\",\"O\").replace(\"I-Disease\",\"O\")}\\n')\n",
    "            f.write(\"\\n\")\n",
    "if TGT_ENTITY_TYPE == \"Disease\":\n",
    "    with open(\"disease_\"+save_path, 'w') as f:\n",
    "        for s in all_samples:\n",
    "            if not any([e[1]==\"B-Disease\" for e in s]):\n",
    "                continue\n",
    "            for t,t_anno in s:\n",
    "                f.write(f'{t}\\t{t_anno.replace(\"B-Chemical\",\"O\").replace(\"I-Chemical\",\"O\")}\\n')\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
